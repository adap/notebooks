{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentially Private Federated Learning with Opacus and Flower\n",
    "\n",
    "In this notebook, we will learn how to use Opacus with PyTorch and Flower for differentially private federated learning. The code is related to other examples, for example, the Flower Intoduction, so it will discuss the topics that were covered in previous notebooks only briefly.\n",
    "\n",
    "Federated learning inherently offers a form of privacy protection by only sending model updates to the server instead of raw client data. However, it has been shown that there are attacks which enable you to, for example, reconstruct parts of the training data or infer the membership of certain users given only the trained model. A popular approach used to mitigate this privacy risk is differential privacy.\n",
    "\n",
    "This post will introduce the concept and demonstrate how to add it to a PyTorch-based Flower client using the [Opacus library](https://opacus.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is differential privacy?\n",
    "​\n",
    "Essentially it's a definition for privacy that allows you to mathematically reason about the risk of an adversary learning information about a user given a model's parameters and arbitrary side information. For a training algorithm to satisfy differential privacy, it needs to ensure that a single user missing from or appearing in a training set won't have too much of an effect on the model.\n",
    "​\n",
    "\n",
    "Formally ([Dwork et al. 2006](https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf)), given two adjacent datasets $d$ and $d'$ - where $d'$ can be formed from $d$ by adding or removing all the samples belonging to a single user - a randomized training algorithm $\\mathcal{A}: \\mathcal{D} \\rightarrow \\mathcal{M}$, mapping possible training datasets to possible trained models, is $(\\varepsilon, \\delta)$-differentially private if for any subset of outputs $O \\in \\mathcal{M}$ we have\n",
    "$$Pr[\\mathcal{A}(d) \\in O] \\leq e^\\varepsilon Pr[\\mathcal{A}(d') \\in O] + \\delta.$$\n",
    "​\n",
    "\n",
    "Here $\\varepsilon$ is the so called privacy budget: the lower it is, the better the privacy guarantee you can provide. The other parameter, $\\delta$ is a measure of the probability that something goes wrong and you can't fulfill that guarantee (and again, the lower the better).\n",
    "​\n",
    "\n",
    "For deep learning, the currently most commonly used algorithm  to achieve $(\\varepsilon, \\delta)$-differential privacy is DP-SGD ([Abadi et al. 2016](https://arxiv.org/pdf/1607.00133.pdf)), which can easily be adapted to be used as part of federated learning strategies such as FedAvg ([McMahan et al. 2017](https://arxiv.org/pdf/1710.06963v1.pdf)).\n",
    "​\n",
    "\n",
    "It has two main steps used to increase user privacy:\n",
    "​\n",
    "- **Gradient norm clipping**, which ensures a single client's influence on the overall average is bounded to a maximum gradient norm of $L$ ($L$ depending on many factors determined by your model and data), and\n",
    "- **Gaussian noising**, which introduces the element of randomness needed by adding $\\mathcal{N}(0, L^2\\sigma^2)$ to a model's parameters ($\\sigma$ is then referred to as the noise multiplier).\n",
    "\n",
    "​\n",
    "The algorithm also relies on clients and data samples being selected uniformly at random.\n",
    "​\n",
    "\n",
    "For federated learning in particular, it is possible to either add noise locally to the model update before sending it to the server, or add noise centrally to the averaged model. The central approach leads to more accurate models since less noise is added overall, however, it relies on the assumption of a trusted server. The following example will focus on local differential privacy.\n",
    "​\n",
    "\n",
    "Another important part of any library providing differentially private mechanisms is to provide a way to keep track of the privacy budget. This is because multiple applications of a mechanism on the same dataset change the privacy guarantees you can provide, as obviously any access to data means potentially revealing more information about a client. Computing $\\varepsilon$ values and bounds is mathematically quite complicated, but broadly speaking every epoch of training performed in a client results in $\\varepsilon$ growing in an additive way, while across your federated setting you will have a form of parallel composition where the maximum $\\varepsilon$ out of all of your clients determines your overall budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Opacus\n",
    "\n",
    "Let's start again by installing and importing the necessary dependencies, including Opacus. The current release of Opacus is not (yet) compatible with the latest version of PyTorch, which is why we install earlier version of both `torch` and `torchvision`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchcsprng==0.1.3+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install matplotlib opacus==0.14.0 torch==1.7.0 torchvision==0.8.0 git+https://github.com/adap/flower.git@release/0.17#egg=flwr[\"simulation\"]\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import List\n",
    "\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import opacus                                           # <-- NEW\n",
    "from opacus import PrivacyEngine                        # <-- NEW\n",
    "from opacus.dp_model_inspector import DPModelInspector  # <-- NEW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print(opacus.__version__)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = \"cpu\"  # Enable this line to force execution on CPU\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our task\n",
    "\n",
    "As before, we begin by defining our basic task. This includes data loading and model architecture, and the usual `get_parameters`/`set_parameters`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 2  # This time, we'll only us two clients\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_datasets():\n",
    "    # Download and transform CIFAR-10 (train and test)\n",
    "    transform = transforms.Compose(\n",
    "      [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = CIFAR10(\"./dataset\", train=True, download=True, transform=transform)\n",
    "    testset = CIFAR10(\"./dataset\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Split training set into 10 partitions to simulate the individual dataset\n",
    "    partition_size = len(trainset) // NUM_CLIENTS\n",
    "    lengths = [partition_size] * NUM_CLIENTS\n",
    "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for ds in datasets:\n",
    "        len_val = len(ds) // 10  # 10 % validation set\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
    "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
    "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets()\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example using Opacus\n",
    "​\n",
    "There are multiple libraries available implementing differentially private machine learning. These are typically dependant on the library you use for training your models, since the clipping step is usually performed with each training step and hence requires you to have library-specific wrappers for optimizers. This example uses Opacus which is specific to PyTorch, however there also exists an example of a [Flower client implementing DP-SGD using Tensorflow Privacy](https://github.com/adap/flower/tree/main/examples/dp-sgd-mnist).\n",
    "​\n",
    "The full code for this example and instructions for running it can be found [here](). It builds on the [PyTorch Quickstart](https://flower.dev/docs/quickstart_pytorch.html) example which trains a convolutional network on the CIFAR10 dataset, so this assumes that you are familiar with it and have the code ready. \n",
    "​\n",
    "The first step is to check whether your network is compatible with Opacus, since it doesn't currently support all layers (see more about this in the [documentation](https://github.com/pytorch/opacus/blob/master/opacus/README.md)). For this you just use the `DPModelInspector` when you instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model():\n",
    "  model = Net()\n",
    "  inspector = DPModelInspector()\n",
    "  print(inspector.validate(model))\n",
    "\n",
    "\n",
    "validate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trickiest part about the setup is finding the right privacy parameters for the privacy engine:\n",
    "​\n",
    "\n",
    "- **Target delta $\\delta$**: should be set to be less than the inverse of the size of the training dataset, so for example if your dataset has 50,000 training points like CIFAR10, a good value to set it to is $10^{-5}$.\n",
    "- **Noise multiplier $\\sigma$**: determines the amount of noise added in each step, the larger it is the smaller the resulting $\\varepsilon$ value.\n",
    "- **Target epsilon $\\varepsilon$**: as an alternative to having a fixed noise multiplier, it can be computed by the engine when given a target $\\varepsilon$. However this requires you to also provide the number of training epochs, which is harder to know in a federated setting since you need to consider both global and local training rounds.\n",
    "- **Maximum gradient norm $L$**: for this parameter - which depends heavily on factors such as model architecture, amount of training data on the client and learning rate - it can be useful to do a grid search since setting it too low can result in high bias, whereas setting it too high might destroy model utility ([Andrew et al. 2021](https://arxiv.org/pdf/1905.03871.pdf)). \n",
    "​\n",
    "\n",
    "​\n",
    "Instead of a sample rate you can also provide both batch size (the number of training samples taken in one step) and sample size (overall size of the dataset in one client), since `sample_rate = batch_size / sample_size`.\n",
    "\n",
    "​\n",
    "There are more advanced parameters you can specify, you can find more details in the Opacus [documentation](https://opacus.ai/api/privacy_engine.html) and their [tutorials](https://opacus.ai/tutorials/).\n",
    "\n",
    "​\n",
    "The example uses the following parameters but they aren't optimal (depending on your preferences in terms of the utility-privacy trade-off):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'batch_size': 32,\n",
    "    'train_split': 0.7,\n",
    "    'local_epochs': 1\n",
    "}\n",
    "PRIVACY_PARAMS = {\n",
    "    'target_delta': 1e-5,\n",
    "    'noise_multiplier': 0.4,\n",
    "    'max_grad_norm': 1.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to attach the privacy engine to you optimizer in each round of training, and optionally get and return the current privacy budget spent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, privacy_engine, epochs):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    # Attach privacy engine to optimizer\n",
    "    privacy_engine.attach(optimizer)\n",
    "    for _ in range(epochs):\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # Get privacy budget\n",
    "    epsilon, _ = optimizer.privacy_engine.get_privacy_spent(PRIVACY_PARAMS['target_delta'])\n",
    "    return epsilon\n",
    "\n",
    "\n",
    "# Same as before\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you specify an instance of the PrivacyEngine in your client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE = {}\n",
    "\n",
    "def get_privacy_engine(cid, model, sample_rate):\n",
    "    if cid not in PE.keys():\n",
    "        PE[cid] = PrivacyEngine(\n",
    "            model,\n",
    "            sample_rate = sample_rate,\n",
    "            target_delta = PRIVACY_PARAMS['target_delta'],\n",
    "            max_grad_norm = PRIVACY_PARAMS['max_grad_norm'],\n",
    "            noise_multiplier = PRIVACY_PARAMS['noise_multiplier']\n",
    "        )\n",
    "    return PE[cid]  # Use the previously created PrivacyEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to only have one engine per model that persists throughout each round of training, since otherwise the privacy tracking won't be accurate. To achieve that whilst still using the Flower Virtual Client Engine for a resource efficient single-machine simulation, we have to use a little trick: we initialize the privacy engine for each client lazily and keep a reference to it in a dictionary. This allows us to use the Virtual Client Engine and discard `FlowerClient` instances after use, but still re-use the same `PrivacyEngine` instance for each client every time it gets created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the FlowerClient\n",
    "\n",
    "You can then also return the privacy budget as a custom metric in the client's fitting method (which can then be further used by [overriding the aggregation strategy](https://flower.dev/docs/saving-progress.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader, privacy_engine):\n",
    "        super().__init__()\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.privacy_engine = privacy_engine\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return get_parameters(self.net)\n",
    "    \n",
    "    def fit(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "        epsilon = train(self.net, self.trainloader, self.privacy_engine, PARAMS['local_epochs'])\n",
    "        print(f\"[CLIENT {self.cid}] epsilon = {epsilon:.2f}\")\n",
    "        return get_parameters(self.net), len(self.trainloader), {\"epsilon\":epsilon}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        print(f\"[CLIENT {self.cid}] loss {loss}, accuraccy {accuracy}\")\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training\n",
    "\n",
    "And that's it, you can run your Flower client just as you are used to before! Let's train for a few rounds of federated learning and see if the accuraccy of our differentially private model still increases: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(cid) -> FlowerClient:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    net = Net().to(DEVICE)\n",
    "\n",
    "    # Load data (CIFAR-10)\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "\n",
    "    # PrivacyEngine\n",
    "    sample_rate = BATCH_SIZE / len(trainloader.dataset) \n",
    "    pe = get_privacy_engine(cid, net, sample_rate)\n",
    "\n",
    "    # Create a  single Flower client representing a single organization\n",
    "    return FlowerClient(cid, net, trainloader, valloader, pe)\n",
    "\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "        fraction_eval=1.0,  # Sample 100% of available clients for evaluation\n",
    "        min_fit_clients=2,  # Never sample less than 10 clients for training\n",
    "        min_eval_clients=2,  # Never sample less than 10 clients for evaluation\n",
    "        min_available_clients=2,  # Wait until all 10 clients are available\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    num_rounds=5,\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and next steps\n",
    "​\n",
    "While implementing a basic setup like this is relatively simple, there are a lot of considerations and open questions when it comes to deploying differentially private models in a federated setting in practice.\n",
    "​\n",
    "\n",
    "Ultimately it comes down to considering the trade-off between utility and privacy. On the one hand, differentially private models take longer to converge, requiring more computation ([McMahan et al. 2017](https://arxiv.org/pdf/1710.06963v1.pdf)), and can result in worse models ([Bagdasaryan et al. 2019](https://proceedings.neurips.cc/paper/2019/file/fc0de4e0396fff257ea362983c2dda5a-Paper.pdf)). On the other hand, even relatively large privacy budgets are beneficial for user privacy ([Thakkar et al. 2020](https://arxiv.org/pdf/2006.07490.pdf)). Hence considering your privacy parameters and assumptions about the required utility and privacy becomes crucial.\n",
    "\n",
    "​\n",
    "For Flower, the next question would be to find ways to train models in a differentially private way such that the mechanism becomes independent from the machine learning library, however even this example will hopefully be a useful first step to experiment with privacy in federated learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
